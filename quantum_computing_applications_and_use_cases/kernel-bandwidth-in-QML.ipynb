{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from quask.core import Ansatz, Kernel, KernelFactory, KernelType\n",
    "from quask.core_implementation.pennylane_kernel import PennylaneKernel\n",
    "from quask.evaluator import HaarEvaluator, LieRankEvaluator\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance of Kernel Bandwidth in QML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion-MNIST Dataset\n",
    "\n",
    "The Fashion-MNIST dataset is an image classification for distinguishing clothing items. We follow the preprocessing of the input data as done in [The power of data in Quantum Machine Learning](https://arxiv.org/abs/2011.01938). We use Principal Component Analysis (PCA) to reduce each image to an $n$-dimensional vector, the so-called **feature** vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Rescale the images from [0,255] to the [0.0,1.0] range.\n",
    "x_train, x_test = x_train/255.0, x_test/255.0\n",
    "\n",
    "print(\"Number of original training examples:\", len(x_train))\n",
    "print(\"Number of original test examples:\", len(x_test))\n",
    "\n",
    "def filter_03(x, y):\n",
    "    keep = (y == 0) | (y == 3)\n",
    "    x, y = x[keep], y[keep]\n",
    "    y = y == 0\n",
    "    return x,y\n",
    "\n",
    "x_train, y_train = filter_03(x_train, y_train)\n",
    "x_test, y_test = filter_03(x_test, y_test)\n",
    "\n",
    "print(\"Number of filtered training examples:\", len(x_train))\n",
    "print(\"Number of filtered test examples:\", len(x_test))\n",
    "\n",
    "print(y_train[0])\n",
    "\n",
    "plt.imshow(x_train[0, :, :])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Import modules\n",
    "# =============================================================================\n",
    "# Import general purpose module(s)\n",
    "import numpy as np\n",
    "\n",
    "# Import specific purpose module(s)\n",
    "import tensorflow as tf \n",
    "\n",
    "# =============================================================================\n",
    "# Create dataset\n",
    "# =============================================================================  \n",
    "def create_dataset(N_TRAIN, features_DIM):\n",
    "    \"\"\"\n",
    "    create dataset\n",
    "\n",
    "    This function loads data from the fashion_mnist and manipulates\n",
    "    data to suit our problem, e.g. normalization.\n",
    "\n",
    "    N_TRAIN: n° of training data to teach our ML model\n",
    "    features_DIM: n° of features will have each datum. It corresponds to\n",
    "                  the number of qubits once the encoding is performed\n",
    "    save: by default will save the dataset created. Can be disabled to \n",
    "          create dummy datasets for testing.\n",
    "\n",
    "    \"\"\"\n",
    "    # print('Loading data...')\n",
    "    # data loading\n",
    "    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "    # convert to float in order to normalize the values of the features\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "\n",
    "    # normalization of the features from [0,255] to [0,1]\n",
    "    X_train /= 255\n",
    "    X_test /= 255\n",
    "    \n",
    "    # select only two classes to have a binary classification problem\n",
    "    # =============================================================================\n",
    "    # filter 0-3\n",
    "    # =============================================================================\n",
    "    def filter_03(x, y):\n",
    "        \"\"\"\n",
    "        filter 0-3\n",
    "\n",
    "        This function keeps only the dresses and t-shorts of the\n",
    "        fashion mnist dataset in order to have only a binary classification problem\n",
    "\n",
    "        x: datapoint vector\n",
    "        y: target/label vector relative to the x vector\n",
    "\n",
    "        \"\"\"\n",
    "        keep = (y == 0) | (y == 3)\n",
    "        x, y = x[keep], y[keep]\n",
    "        y = y == 0\n",
    "        return x,y\n",
    "    \n",
    "    # apply the filter \n",
    "    X_train, y_train = filter_03(X_train, y_train)\n",
    "    X_test, y_test = filter_03(X_test, y_test)\n",
    "\n",
    "    # print(\"Number of filtered training examples:\", len(X_train))\n",
    "    # print(\"Number of filtered test examples:\", len(X_test))\n",
    "    \n",
    "    # # to_categorical allows to convert each target value in a vector of len 2\n",
    "    # # e.g. for the first class we have [1,0] and for the second one [0,1]\n",
    "    # y_train = tf.keras.utils.to_categorical(y_train, 2)\n",
    "    # y_test = tf.keras.utils.to_categorical(y_test, 2)\n",
    "\n",
    "    # =============================================================================\n",
    "    # Truncate x\n",
    "    # =============================================================================\n",
    "    def truncate_x(X_train, X_test, n_components=features_DIM):\n",
    "        \"\"\"\n",
    "        truncate x\n",
    "\n",
    "        Perform PCA on image dataset keeping the top `n_components` components.\n",
    "        \n",
    "        X_train: training dataset\n",
    "        X_test: test dataset\n",
    "        n_components: number of features we want to keep after PCA\n",
    "\n",
    "        \"\"\"\n",
    "        n_points_train = tf.gather(tf.shape(X_train), 0)\n",
    "        n_points_test = tf.gather(tf.shape(X_test), 0)\n",
    "\n",
    "        # Flatten to 1D\n",
    "        X_train = tf.reshape(X_train, [n_points_train, -1])\n",
    "        X_test = tf.reshape(X_test, [n_points_test, -1])\n",
    "\n",
    "        # Normalize.\n",
    "        feature_mean = tf.reduce_mean(X_train, axis=0)\n",
    "        X_train_normalized = X_train - feature_mean\n",
    "        X_test_normalized = X_test - feature_mean\n",
    "\n",
    "        # Truncate.\n",
    "        e_values, e_vectors = tf.linalg.eigh(\n",
    "          tf.einsum('ji,jk->ik', X_train_normalized, X_train_normalized))\n",
    "        return tf.einsum('ij,jk->ik', X_train_normalized, e_vectors[:,-n_components:]), \\\n",
    "          tf.einsum('ij,jk->ik', X_test_normalized, e_vectors[:, -n_components:])\n",
    "\n",
    "    # truncating datasets\n",
    "    X_train, X_test = truncate_x(X_train, X_test, n_components=features_DIM)\n",
    "    \n",
    "    print(f'New datapoint dimension:', len(X_train[0]))    \n",
    "\n",
    "    # restrict the dataset to a smaller ensemble\n",
    "    N_TEST = round(N_TRAIN*0.2)\n",
    "    X_train, X_test = X_train[:N_TRAIN], X_test[:N_TEST]\n",
    "    y_train, y_test = y_train[:N_TRAIN], y_test[:N_TEST]\n",
    "\n",
    "    print('Number of datapoints', len(X_train))\n",
    "    \n",
    "    X_train = np.reshape(X_train, (len(X_train), features_DIM))\n",
    "    X_test = np.reshape(X_test, (len(X_test), features_DIM))\n",
    "\n",
    "    print('Data loaded.')\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = create_dataset(N_TRAIN=800, features_DIM=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Quantum Feature Map\n",
    "\n",
    "Actually, the number of features for one image (thus, one circuit embedding) could be bigger than the number of qubits $d$ according to the feature map we pick. In this case we again refer to a well known feature map called Instantaneous Quantum Polynomial (IQP) feature map. \n",
    "\n",
    "$$|\\textbf{x}_i\\rangle = U_{Z}(\\textbf{x}_i) H^{\\otimes d} U_{Z}(\\textbf{x}_i) H^{\\otimes d}|0^d\\rangle$$\n",
    "\n",
    "where\n",
    "\n",
    "$$ U_{Z}(\\textbf{x}_i) = \\text{exp} \\left( \\sum_{j=1}^d \\lambda x_{ij}Z_j + \\sum_{j=1}^d \\sum_{j'=1}^d \\lambda^2 x_{ij} x_{ij'} Z_j Z_{j'} \\right) \\quad . $$\n",
    "\n",
    "Then, our feature array contains both the single-qubit and two-qubit features (the product of the single-qubit features pair entangled by the map scheme).\n",
    "\n",
    "$$ \\textbf{x}_i = (x_{i1}\\;, x_{i2}\\;, ..., x_{id}\\;, x_{i1}x_{i2}\\;, x_{i1}x_{i3}\\;, ..., x_{id-1}x_{id}\\;) .$$\n",
    "\n",
    "The number of additional features depends on the type of entanglement we choose. A full entanglement brings to an additional number of features equal to $\\binom{d}{2}$. Finally, the total number of features for a single datapoint (as in, image) equals to $d + \\binom{d}{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pennylane_noiseless(ansatz: Ansatz, measurement: str, type: KernelType):\n",
    "    return PennylaneKernel(ansatz, measurement, type, device_name=\"default.qubit\", n_shots=None)\n",
    "\n",
    "KernelFactory.add_implementation('pennylane_noiseless', create_pennylane_noiseless)\n",
    "KernelFactory.set_current_implementation('pennylane_noiseless')\n",
    "print(KernelFactory._KernelFactory__implementations)\n",
    "print(KernelFactory._KernelFactory__current_implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UZCirquit(ansatz, bandwidth, count=0):\n",
    "    d = ansatz.n_qubits\n",
    "    # Z term - local\n",
    "    for k in range(d):\n",
    "        ansatz.change_operation(count, new_feature=k, new_wires=[k, (k+1)%d], new_generator=\"ZI\", new_bandwidth=bandwidth)\n",
    "        count += 1\n",
    "\n",
    "    # ZZ term - full entanglement \n",
    "    for i in range(d-1):\n",
    "        for j in range(i+1, d):\n",
    "            ansatz.change_operation(count, new_feature=k+i, new_wires=[i, j], new_generator=\"ZZ\", new_bandwidth=bandwidth)\n",
    "            count += 1\n",
    "\n",
    "    return ansatz, count\n",
    "\n",
    "def HCircuit(ansatz, bandwidth=1, count=0):\n",
    "    d = ansatz.n_qubits\n",
    "    n_features = d + d*(d-1)/2 + 1 # we have the last feature fixed to one for non-parametrizable gates\n",
    "    for i in range(d):\n",
    "        ansatz.change_operation(count, new_feature=n_features, new_wires=[i, (i+1)%d], new_generator=\"XI\", new_bandwidth=np.pi/2) # R_x(pi/2)\n",
    "        ansatz.change_operation(count, new_feature=n_features, new_wires=[i, (i+1)%d], new_generator=\"ZI\", new_bandwidth=np.pi/2) # R_z(pi/2)\n",
    "        ansatz.change_operation(count, new_feature=n_features, new_wires=[i, (i+1)%d], new_generator=\"XI\", new_bandwidth=np.pi/2) # R_x(pi/2)\n",
    "        ansatz.change_operation(count, new_feature=n_features, new_wires=[i, (i+1)%d], new_generator=\"II\", new_bandwidth=np.pi)   # e^(i pi/2)\n",
    "\n",
    "    return ansatz, count\n",
    "\n",
    "def IQPCirquit(ansatz, bandwidth):\n",
    "    # first U_Z(x_i)\n",
    "    ansatz_uz1, count_uz1 = UZCirquit(ansatz, bandwidth)\n",
    "    # first layer of hadamard gates\n",
    "    ansatz_uz1h1, count_uz1h1 = HCircuit(ansatz_uz1, bandwidth, count=count_uz1)\n",
    "    # second U_Z(x_i)\n",
    "    ansatz_uz1h1uz2, count_uz1h1uz2 = UZCirquit(ansatz_uz1h1, bandwidth, count=count_uz1h1)\n",
    "    # second layer of hadamard gates\n",
    "    ansatz_uz1h1uz2h2, _ = HCircuit(ansatz_uz1h1uz2, bandwidth, count=count_uz1h1uz2)\n",
    "    return ansatz_uz1h1uz2h2\n",
    "\n",
    "# we can define a new function to construct the Ansatz since we will loop for different bandwidth values\n",
    "def VaryingBandwidthKernel(N_FEATURES, N_QUBITS, N_OPERATIONS, bandwidth):\n",
    "    ansatz = Ansatz(n_features=N_FEATURES, n_qubits=N_QUBITS, n_operations=N_OPERATIONS)\n",
    "    ansatz.initialize_to_identity()\n",
    "    # we choose a IQPCircuit\n",
    "    ansatz = IQPCirquit(ansatz, bandwidth)\n",
    "    kernel = KernelFactory.create_kernel(ansatz, \"Z\" * N_QUBITS, KernelType.FIDELITY)\n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding the datapoints\n",
    "\n",
    "The number of operations we have to perform following the `IPQCircuit` map are:\n",
    "\n",
    "$$ \\underbrace{\\left(d + \\frac{d(d-1)}{2} \\right)}_{U_{Z}(\\textbf{x}_i)} + \\underbrace{d}_{H^{\\otimes d}} + \\underbrace{\\left(d + \\frac{d(d-1)}{2} \\right)}_{U_{Z}(\\textbf{x}_i)} + \\underbrace{d}_{H^{\\otimes d}} $$\n",
    "\n",
    "which is simpy:\n",
    "\n",
    "$$ d^2 + 3 d \\quad .$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the datapoints for quask\n",
    "\n",
    "def quask_features(data):\n",
    "    new_data = []\n",
    "    for datapoint in data:\n",
    "        new_datapoint = datapoint\n",
    "        for i in range(len(datapoint)-1):\n",
    "            for j in range(i+1, len(datapoint)):\n",
    "                new_datapoint = np.append(new_datapoint, datapoint[i]*datapoint[j])\n",
    "        new_datapoint = np.append(new_datapoint, 1)\n",
    "        new_data.append(new_datapoint)\n",
    "    new_data = np.array(new_data)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_qubits = [2,4]\n",
    "# let us sweep for different values of the bandwidth\n",
    "bandwidths = np.linspace(1e-3,1e+0, 10).tolist()\n",
    "\n",
    "def accuracy_computation(K_train, K_test, y_train, y_test):\n",
    "    # Train the SVC classifier using the precomputed kernel matrices\n",
    "    svc_kern = SVC(kernel='precomputed')\n",
    "    svc_kern.fit(K_train, y_train)\n",
    "\n",
    "    # Predict and evaluate accuracy on the test set\n",
    "    y_pred = svc_kern.predict(K_test)\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "accuracy_qubit = []\n",
    "for d in num_qubits:\n",
    "    features_quask = d + d*(d-1)/2 + 1\n",
    "    operations_quask = d**2 + 3*d \n",
    "\n",
    "    X_train, X_test, y_train, y_test = create_dataset(N_TRAIN=100, features_DIM=d)\n",
    "    print(X_train.shape)\n",
    "    X_train = quask_features(X_train)\n",
    "    X_test = quask_features(X_test)\n",
    "    print(X_train.shape, features_quask)\n",
    "    accuracy_beta = []\n",
    "    for beta in bandwidths:\n",
    "        print(f\"Qubits: {d}, beta: {beta}\")\n",
    "        kernel = VaryingBandwidthKernel(N_FEATURES=features_quask, N_QUBITS=d, N_OPERATIONS=operations_quask, bandwidth=beta)\n",
    "        K_train = kernel.build_kernel(X_train, X_train)\n",
    "        K_test = kernel.build_kernel(X_test, X_train)\n",
    "        accuracy = accuracy_computation(K_train, K_test, y_train, y_test)\n",
    "        accuracy_beta.append(accuracy)\n",
    "    accuracy_qubit.append(accuracy_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for accuracy_beta, d in zip(accuracy_qubit, num_qubits):\n",
    "    plt.plot(bandwidths, accuracy_beta, label=f\"{d}\")\n",
    "plt.xscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_qubits = [3,5,7,9,11]\n",
    "# let us sweep for different values of the bandwidth\n",
    "bandwidths = np.linspace(1e-3,1e+0, 10).tolist()\n",
    "cost_he_qubit = []\n",
    "cost_lre_qubit = []\n",
    "for d in num_qubits:\n",
    "    features_quask = int(d + d*(d-1)/2 + 1)\n",
    "    operations_quask = int(d**2 + 3*d)\n",
    "    cost_he_beta = []\n",
    "    cost_lre_beta = []\n",
    "    for beta in bandwidths:\n",
    "        print(f\"Qubits: {d}, beta: {beta}\")\n",
    "        kernel = VaryingBandwidthKernel(N_FEATURES=features_quask, N_QUBITS=d, N_OPERATIONS=operations_quask, bandwidth=beta)\n",
    "        he = HaarEvaluator(n_bins=40, n_samples=10000)\n",
    "        cost_he = he.evaluate(kernel=kernel, K=None, X=None, y=None)\n",
    "        lre = LieRankEvaluator(T=500)\n",
    "        cost_lre = lre.evaluate(kernel=kernel, K=None, X=None, y=None)\n",
    "        cost_he_beta.append(cost_he)\n",
    "        cost_lre_beta.append(cost_lre)\n",
    "    cost_he_qubit.append(cost_he_beta)\n",
    "    cost_lre_qubit.append(cost_lre_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cost_he, d in zip(cost_he_qubit, num_qubits):\n",
    "    plt.plot(bandwidths, cost_he, label=f\"{d}\")\n",
    "plt.xscale('log')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olqti24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
